from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
import torch
import json
from datetime import datetime
import os

# Embedding modeli ve tokenizer
embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)
embedding_model = AutoModel.from_pretrained(embedding_model_name)

# Cevap Ã¼retim modeli ve tokenizer
generation_model_name = "microsoft/DialoGPT-medium"
generation_tokenizer = AutoTokenizer.from_pretrained(generation_model_name)
generation_model = AutoModelForCausalLM.from_pretrained(generation_model_name)

# Pad token kontrolÃ¼
if generation_tokenizer.pad_token is None:
    generation_tokenizer.pad_token = generation_tokenizer.eos_token

# Cihaz seÃ§imi
device = 'cuda' if torch.cuda.is_available() else 'cpu'
embedding_model.to(device)
generation_model.to(device)

# Log dosyasÄ±
LOG_FILE = "user_query_logs.json"

# KullanÄ±cÄ± sorgularÄ±nÄ± loglama
def log_user_query(query, recommendations, response_text):
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "query": query,
        "recommendations": [r[0] for r in recommendations],
        "response": response_text
    }
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r") as f:
            data = json.load(f)
    else:
        data = []

    data.append(log_entry)

    with open(LOG_FILE, "w") as f:
        json.dump(data, f, indent=2)

# Sohbet geÃ§miÅŸini yÃ¼kleme
def load_chat_history(n=3):
    if not os.path.exists(LOG_FILE):
        return ""
    with open(LOG_FILE, "r") as f:
        data = json.load(f)[-n:]  # Son n kaydÄ± al
    history = ""
    for entry in data:
        history += f"KullanÄ±cÄ±: {entry['query']}\nSistem: {entry['response']}\n"
    return history.strip()

# Few-shot Ã¶rnekleri
FEW_SHOT_EXAMPLES = """
KullanÄ±cÄ±: Aksiyon ve bilim kurgu karÄ±ÅŸÄ±mÄ± bir film arÄ±yorum.
Sistem: Ä°ÅŸte tam sana gÃ¶re iÃ§erikler:
1. The Matrix: Sanal gerÃ§eklik dÃ¼nyasÄ±nda geÃ§en, felsefi ve aksiyon dolu bir film. 
Bu filmler zihin aÃ§Ä±cÄ± ve sÃ¼rÃ¼kleyici, keyifli seyirler!

KullanÄ±cÄ±: Romantik ama komik bir film Ã¶nerir misin?
Sistem: Elbette! AÅŸaÄŸÄ±daki iÃ§erikler tam senlik:
1. Crazy Rich Asians: LÃ¼ks ve aÅŸk dolu bir yolculuk. 
2. The Proposal: Patron ve Ã§alÄ±ÅŸan arasÄ±nda sÃ¼rprizlerle dolu bir evlilik planÄ±. 
GÃ¼lÃ¼mseten bir aÅŸk hikayesi arÄ±yorsan, bu filmler birebir.
"""

# Ã–neri sistemi
def get_recommendations(user_query, df, embeddings, top_k=5):
    # KullanÄ±cÄ± sorgusunu embedding'e dÃ¶nÃ¼ÅŸtÃ¼rme
    inputs = embedding_tokenizer(user_query, return_tensors="pt", truncation=True, padding=True)
    inputs = {key: value.to(device) for key, value in inputs.items()}
    with torch.no_grad():
        outputs = embedding_model(**inputs)
        query_embedding = outputs.last_hidden_state.mean(dim=1)

    # Embedding'ler ile benzerlik hesaplama
    embeddings_tensor = torch.tensor(embeddings).to(device)
    cos_scores = torch.cosine_similarity(query_embedding, embeddings_tensor, dim=1)
    top_results = torch.topk(cos_scores, k=top_k)

    # En iyi sonuÃ§larÄ± toplama
    recommendations = []
    for score, idx in zip(top_results.values, top_results.indices):
        idx = idx.item()
        title = df.iloc[idx]["title"]
        desc = df.iloc[idx]["description"]
        fake_link = f"https://netflix.com/watch/{idx}"
        recommendations.append((title, desc, score.item(), fake_link))

    # Ã–nerileri birleÅŸtirme
    joined_recommendations = "\n".join([
        f"{i+1}. {title}: {desc} (ğŸ“º {link})"
        for i, (title, desc, _, link) in enumerate(recommendations)
    ])

    # Prompt oluÅŸturma (sadece mevcut sorgu iÃ§in)
    prompt = f"KullanÄ±cÄ±: {user_query}\nSistem: Ä°ÅŸte Ã¶nerilerim:\n{joined_recommendations}\n"

    # Cevap Ã¼retimi
    gen_inputs = generation_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=1024)
    gen_inputs = {key: value.to(device) for key, value in gen_inputs.items()}
    with torch.no_grad():
        generated_ids = generation_model.generate(
            gen_inputs["input_ids"],
            max_new_tokens=50,
            num_return_sequences=1,
            pad_token_id=generation_tokenizer.pad_token_id,
            attention_mask=gen_inputs["attention_mask"]
        )

    response_text = generation_tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    # KullanÄ±cÄ± sorgusunu loglama
    log_user_query(user_query, recommendations, response_text)

    # Sadece kullanÄ±cÄ± sorgusu ve model cevabÄ±nÄ± dÃ¶ndÃ¼rme
    return f"KullanÄ±cÄ±: {user_query}\nSistem: {response_text}"