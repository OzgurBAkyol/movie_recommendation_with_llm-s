from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
import torch
import json
from datetime import datetime
import os

# === MODELLER === #
embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)
embedding_model = AutoModel.from_pretrained(embedding_model_name)

generation_model_name = "microsoft/DialoGPT-medium"
generation_tokenizer = AutoTokenizer.from_pretrained(generation_model_name)
generation_model = AutoModelForCausalLM.from_pretrained(generation_model_name)

if generation_tokenizer.pad_token is None:
    generation_tokenizer.pad_token = generation_tokenizer.eos_token

device = 'cuda' if torch.cuda.is_available() else 'cpu'
embedding_model.to(device)
generation_model.to(device)

# === LOGGING === #
LOG_FILE = "user_query_logs.json"

def log_user_query(query, recommendations, response_text):
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "query": query,
        "recommendations": [r[0] for r in recommendations],
        "response": response_text
    }
    if os.path.exists(LOG_FILE):
        with open(LOG_FILE, "r") as f:
            data = json.load(f)
    else:
        data = []

    data.append(log_entry)

    with open(LOG_FILE, "w") as f:
        json.dump(data, f, indent=2)

def load_chat_history(n=3):
    if not os.path.exists(LOG_FILE):
        return ""
    with open(LOG_FILE, "r") as f:
        data = json.load(f)[-n:]  # Son n kaydÄ± al
    history = ""
    for entry in data:
        history += f"KullanÄ±cÄ±: {entry['query']}\nSistem: {entry['response']}\n"
    return history.strip()

# === FEW-SHOT PROMPT Ã–RNEÄÄ° === #
FEW_SHOT_EXAMPLES = """
KullanÄ±cÄ±: Aksiyon ve bilim kurgu karÄ±ÅŸÄ±mÄ± bir film arÄ±yorum.
Sistem: Ä°ÅŸte tam sana gÃ¶re iÃ§erikler:
1. The Matrix: Sanal gerÃ§eklik dÃ¼nyasÄ±nda geÃ§en, felsefi ve aksiyon dolu bir film. (ğŸ“º https://netflix.com/watch/0)
2. Inception: RÃ¼ya iÃ§inde rÃ¼ya konseptiyle bilinÃ§altÄ±nda geÃ§en bir gÃ¶rev. (ğŸ“º https://netflix.com/watch/1)
Bu filmler zihin aÃ§Ä±cÄ± ve sÃ¼rÃ¼kleyici, keyifli seyirler!

KullanÄ±cÄ±: Romantik ama komik bir film Ã¶nerir misin?
Sistem: Elbette! AÅŸaÄŸÄ±daki iÃ§erikler tam senlik:
1. Crazy Rich Asians: LÃ¼ks ve aÅŸk dolu bir yolculuk. (ğŸ“º https://netflix.com/watch/2)
2. The Proposal: Patron ve Ã§alÄ±ÅŸan arasÄ±nda sÃ¼rprizlerle dolu bir evlilik planÄ±. (ğŸ“º https://netflix.com/watch/3)
GÃ¼lÃ¼mseten bir aÅŸk hikayesi arÄ±yorsan, bu filmler birebir.
"""

# === ANA FONKSÄ°YON === #
# === ANA FONKSÄ°YON === #
def get_recommendations(user_query, df, embeddings, top_k=5):
    # Embed sorgu
    inputs = embedding_tokenizer(user_query, return_tensors="pt", truncation=True, padding=True)
    inputs = {key: value.to(device) for key, value in inputs.items()}
    with torch.no_grad():
        outputs = embedding_model(**inputs)
        query_embedding = outputs.last_hidden_state.mean(dim=1)

    # Embed karÅŸÄ±laÅŸtÄ±rmasÄ±
    embeddings_tensor = torch.tensor(embeddings).to(device)
    cos_scores = torch.cosine_similarity(query_embedding, embeddings_tensor, dim=1)
    top_results = torch.topk(cos_scores, k=top_k)

    # Tavsiyeleri topla
    recommendations = []
    for score, idx in zip(top_results.values, top_results.indices):
        idx = idx.item()
        title = df.iloc[idx]["title"]
        desc = df.iloc[idx]["description"]
        fake_link = f"https://netflix.com/watch/{idx}"
        recommendations.append((title, desc, score.item(), fake_link))

    # Ã–nerileri prompt'a hazÄ±rla
    joined_recommendations = "\n".join([
        f"{i+1}. {title}: {desc} (ğŸ“º {link})"
        for i, (title, desc, _, link) in enumerate(recommendations)
    ])

    # Chat geÃ§miÅŸi + few-shot + kullanÄ±cÄ± giriÅŸi
    chat_history = load_chat_history()
    prompt = (
        FEW_SHOT_EXAMPLES.strip() + "\n\n" +
        chat_history + ("\n\n" if chat_history else "") +
        f"KullanÄ±cÄ±: {user_query}\nSistem: Ä°ÅŸte Ã¶nerilerim:\n{joined_recommendations}\n"
        "Bu Ã¶nerileri kÄ±sa ve samimi ÅŸekilde Ã¶zetle."
    )

    # YanÄ±t Ã¼retimi
    gen_inputs = generation_tokenizer(prompt, return_tensors="pt", truncation=True, padding=True, max_length=1024)
    gen_inputs = {key: value.to(device) for key, value in gen_inputs.items()}
    with torch.no_grad():
        generated_ids = generation_model.generate(
            gen_inputs["input_ids"],
            max_new_tokens=50,  # Daha kÄ±sa yanÄ±tlar Ã¼retmek iÃ§in deÄŸeri 50'ye dÃ¼ÅŸÃ¼rdÃ¼m.
            num_return_sequences=1,
            pad_token_id=generation_tokenizer.pad_token_id,
            attention_mask=gen_inputs["attention_mask"]
        )

    response_text = generation_tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    # YalnÄ±zca kullanÄ±cÄ± sorusu ve cevabÄ± dÃ¶ndÃ¼rÃ¼lÃ¼r
    return f"KullanÄ±cÄ±: {user_query}\nSistem: {response_text}"
